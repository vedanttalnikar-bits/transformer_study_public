{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_head_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTtRYhVaifGFcbIlxEstuK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedanttalnikar-bits/transformer_study_public/blob/main/multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjJJyJTZYebt"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmzGPEy64qmA"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz5BMC8Kaoqo"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
        "\n",
        "\n",
        "Multi-head attention consists of four parts:\n",
        "*    Linear layers and split into heads.\n",
        "*    Scaled dot-product attention.\n",
        "*    Concatenation of heads.\n",
        "*    Final linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPmbr6F1C-v_"
      },
      "source": [
        "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
        "\n",
        "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
        "\n",
        "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdmGinqLLyNU"
      },
      "source": [
        "def calc_mul_add(q, k):\r\n",
        "  # mul = depth * (seq_len_q * seq_len_k)\r\n",
        "  # add = (depth - 1) * (seq_len_q * seq_len_k))\r\n",
        "\r\n",
        "  #depth\r\n",
        "  dk = tf.cast(tf.shape(k)[3], tf.float32)\r\n",
        "\r\n",
        "  seq_len_k = tf.cast(tf.shape(k)[2], tf.float32)\r\n",
        "  seq_len_q = tf.cast(tf.shape(q)[2], tf.float32)\r\n",
        "  batch_size = tf.cast(tf.shape(q)[0],tf.float32)\r\n",
        "  heads = tf.cast(tf.shape(q)[1],tf.float32)\r\n",
        "\r\n",
        "  mul = dk * (seq_len_q * seq_len_k)\r\n",
        "  add = (dk - 1 ) * (seq_len_q * seq_len_k)\r\n",
        "  #tf.print(heads, batch_size, seq_len_k, seq_len_q,dk)\r\n",
        "  total_mul = batch_size * (mul * heads) \r\n",
        "  total_add = batch_size * (add * heads) \r\n",
        "  tf.print(\"Activation vs Activation per head:\")\r\n",
        "  tf.print(\"Multiplications:\", mul)\r\n",
        "  tf.print(\"Additions:\", add)\r\n",
        "  tf.print(\"Total Mul = \", total_mul  )\r\n",
        "  tf.print(\"Total add = \", total_add  )\r\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKEFvIQENd5z"
      },
      "source": [
        "def calc_mul_add2(w, v):\r\n",
        "  # mul = depth * (seq_len_q * seq_len_k)\r\n",
        "  # add = (depth - 1) * (seq_len_q * seq_len_k))\r\n",
        "\r\n",
        "  #depth\r\n",
        "  dk = tf.cast(tf.shape(w)[3], tf.float32)\r\n",
        "  #tf.print(v.shape)\r\n",
        "  #tf.print(w.shape)\r\n",
        "  #tf.print(dk)\r\n",
        "  seq_len_v = tf.cast(tf.shape(v)[2], tf.float32)\r\n",
        "  seq_len_w = tf.cast(tf.shape(w)[2], tf.float32)\r\n",
        "  batch_size = tf.cast(tf.shape(w)[0],tf.float32)\r\n",
        "  heads = tf.cast(tf.shape(w)[1],tf.float32)\r\n",
        "\r\n",
        "  mul = dk * (seq_len_w * seq_len_v)\r\n",
        "  add = (dk - 1 ) * (seq_len_w * seq_len_v)\r\n",
        "\r\n",
        "  tf.print(\"Weight vs Activation:\")\r\n",
        "  tf.print(\"Multiplications:\", mul )\r\n",
        "  tf.print(\"Additions:\", add)\r\n",
        "\r\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LazzUq3bJ5SH"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  calc_mul_add(q,k)\n",
        "  #tf.print(q.shape, k.shape)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += m  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "  calc_mul_add2(attention_weights,v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSV3PPKsYecw"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    #tf.print(\"output before concat:\", scaled_attention.shape)\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    #tf.print(\"post concatation:\", concat_attention.shape)\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "    tf.print(output.shape)\n",
        "    return output, attention_weights"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D8FJue5lDyZ"
      },
      "source": [
        "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu94p-_-2_BX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae1b698-d7a3-414c-e999-df1998d4fb1c"
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Activation vs Activation per head:\n",
            "Multiplications: 230400\n",
            "Additions: 226800\n",
            "Total Mul =  1.8432e+06\n",
            "Total add =  1.8144e+06\n",
            "Weight vs Activation:\n",
            "Multiplications: 216000\n",
            "Additions: 212400\n",
            "shape_dense: TensorShape([1, 8, 60, 64])\n",
            "shape_before_dense: TensorShape([1, 60, 512])\n",
            "TensorShape([1, 60, 512])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    }
  ]
}