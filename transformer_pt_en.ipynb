{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLDAFcNMNbtchiAby6jYDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedanttalnikar-bits/transformer_study_public/blob/main/transformer_pt_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFG0NDRu5mYQ"
      },
      "source": [
        "!pip install -q tfds-nightly\n",
        "\n",
        "# Pin matplotlib version to 3.2.2 since in the latest version\n",
        "# transformer.ipynb fails with the following error:\n",
        "# https://stackoverflow.com/questions/62953704/valueerror-the-number-of-fixedlocator-locations-5-usually-from-a-call-to-set\n",
        "!pip install matplotlib==3.2.2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epj64kDfu0HQ"
      },
      "source": [
        "!pip install pandas\r\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-CGyIwhv54p"
      },
      "source": [
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Ytzdx94KM6"
      },
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\r\n",
        "                               as_supervised=True)\r\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTPTT5eV7Xjx"
      },
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\r\n",
        "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\r\n",
        "\r\n",
        "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\r\n",
        "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUf6JAao8PmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4d5cd9-118d-4009-a599-d00ad18ce507"
      },
      "source": [
        "sample_string = 'Transformer is awesome.'\r\n",
        "\r\n",
        "tokenized_string = tokenizer_en.encode(sample_string)\r\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\r\n",
        "\r\n",
        "original_string = tokenizer_en.decode(tokenized_string)\r\n",
        "print ('The original string: {}'.format(original_string))\r\n",
        "\r\n",
        "assert original_string == sample_string"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
            "The original string: Transformer is awesome.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf2ntBxjkqK6"
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riw3xJ8M8X06"
      },
      "source": [
        "BUFFER_SIZE = 20000\r\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GDKs8Hl-eOK"
      },
      "source": [
        "def encode(lang1, lang2):\r\n",
        "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\r\n",
        "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\r\n",
        "\r\n",
        "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\r\n",
        "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\r\n",
        "  \r\n",
        "  return lang1, lang2"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPVesZ76-hPy"
      },
      "source": [
        "def tf_encode(pt, en):\r\n",
        "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\r\n",
        "  result_pt.set_shape([None])\r\n",
        "  result_en.set_shape([None])\r\n",
        "\r\n",
        "  return result_pt, result_en"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN0SWcFN-t_6"
      },
      "source": [
        "MAX_LENGTH = 40"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qf-Iw6Q-0oi"
      },
      "source": [
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\r\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\r\n",
        "                        tf.size(y) <= max_length)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJiqmRNq-6Fi"
      },
      "source": [
        "train_dataset = train_examples.map(tf_encode)\r\n",
        "train_dataset = train_dataset.filter(filter_max_length)\r\n",
        "# cache the dataset to memory to get a speedup while reading from it.\r\n",
        "train_dataset = train_dataset.cache()\r\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\r\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "\r\n",
        "val_dataset = val_examples.map(tf_encode)\r\n",
        "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85Og5SGO--DK"
      },
      "source": [
        "pt_batch, en_batch = next(iter(val_dataset))\r\n",
        "pt_batch, en_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad4l9IV7_HOK"
      },
      "source": [
        "def get_angles(pos, i, d_model):\r\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\r\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn_XLRBf_Jhi"
      },
      "source": [
        "def positional_encoding(position, d_model):\r\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\r\n",
        "                          np.arange(d_model)[np.newaxis, :],\r\n",
        "                          d_model)\r\n",
        "  \r\n",
        "  # apply sin to even indices in the array; 2i\r\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\r\n",
        "  \r\n",
        "  # apply cos to odd indices in the array; 2i+1\r\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\r\n",
        "    \r\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\r\n",
        "    \r\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNg3-Tdr_SbC"
      },
      "source": [
        "pos_encoding = positional_encoding(50, 512)\r\n",
        "print (pos_encoding.shape)\r\n",
        "\r\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\r\n",
        "plt.xlabel('Depth')\r\n",
        "plt.xlim((0, 512))\r\n",
        "plt.ylabel('Position')\r\n",
        "plt.colorbar()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9hb7CA7_bhq"
      },
      "source": [
        "def create_padding_mask(seq):\r\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\r\n",
        "  \r\n",
        "  # add extra dimensions to add the padding\r\n",
        "  # to the attention logits.\r\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7uOHyCW_f6q"
      },
      "source": [
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\r\n",
        "create_padding_mask(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrkYFjLn_kXi"
      },
      "source": [
        "def create_look_ahead_mask(size):\r\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\r\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS5xTUVa_op6"
      },
      "source": [
        "x = tf.random.uniform((1, 3))\r\n",
        "temp = create_look_ahead_mask(x.shape[1])\r\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usz_hoBJ_xKS"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\r\n",
        "  \"\"\"Calculate the attention weights.\r\n",
        "  q, k, v must have matching leading dimensions.\r\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\r\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \r\n",
        "  but it must be broadcastable for addition.\r\n",
        "  \r\n",
        "  Args:\r\n",
        "    q: query shape == (..., seq_len_q, depth)\r\n",
        "    k: key shape == (..., seq_len_k, depth)\r\n",
        "    v: value shape == (..., seq_len_v, depth_v)\r\n",
        "    mask: Float tensor with shape broadcastable \r\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\r\n",
        "    \r\n",
        "  Returns:\r\n",
        "    output, attention_weights\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\r\n",
        "  \r\n",
        "  # scale matmul_qk\r\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\r\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\r\n",
        "\r\n",
        "  # add the mask to the scaled tensor.\r\n",
        "  if mask is not None:\r\n",
        "    scaled_attention_logits += (mask * -1e9)  \r\n",
        "\r\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\r\n",
        "  # add up to 1.\r\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\r\n",
        "\r\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\r\n",
        "\r\n",
        "  return output, attention_weights"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h66JxxqtAAk5"
      },
      "source": [
        "def print_out(q, k, v):\r\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\r\n",
        "      q, k, v, None)\r\n",
        "  print ('Attention weights are:')\r\n",
        "  print (temp_attn)\r\n",
        "  print ('Output is:')\r\n",
        "  print (temp_out)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjcjfZQfAO2L"
      },
      "source": [
        "np.set_printoptions(suppress=True)\r\n",
        "\r\n",
        "temp_k = tf.constant([[10,0,0],\r\n",
        "                      [0,10,0],\r\n",
        "                      [0,0,10],\r\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\r\n",
        "\r\n",
        "temp_v = tf.constant([[   1,0],\r\n",
        "                      [  10,0],\r\n",
        "                      [ 100,5],\r\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\r\n",
        "\r\n",
        "# This `query` aligns with the second `key`,\r\n",
        "# so the second `value` is returned.\r\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\r\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz-f43QzAhOj",
        "outputId": "b3984d8e-91ab-4d62-e600-2422684f5df1"
      },
      "source": [
        "# This query aligns with a repeated key (third and fourth), \r\n",
        "# so all associated values get averaged.\r\n",
        "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\r\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgIgaD9BAnDb",
        "outputId": "398a1e51-a20f-449e-f05a-1ad5d9738af8"
      },
      "source": [
        "# This query aligns equally with the first and second key, \r\n",
        "# so their values get averaged.\r\n",
        "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\r\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elyi785RArND"
      },
      "source": [
        "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\r\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTSdJpl2AyXb"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, d_model, num_heads):\r\n",
        "    super(MultiHeadAttention, self).__init__()\r\n",
        "    self.num_heads = num_heads\r\n",
        "    self.d_model = d_model\r\n",
        "    \r\n",
        "    assert d_model % self.num_heads == 0\r\n",
        "    \r\n",
        "    self.depth = d_model // self.num_heads\r\n",
        "    \r\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\r\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\r\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\r\n",
        "    \r\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\r\n",
        "        \r\n",
        "  def split_heads(self, x, batch_size):\r\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\r\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\r\n",
        "    \"\"\"\r\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\r\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\r\n",
        "    \r\n",
        "  def call(self, v, k, q, mask):\r\n",
        "    batch_size = tf.shape(q)[0]\r\n",
        "    \r\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\r\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\r\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\r\n",
        "    \r\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\r\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\r\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\r\n",
        "    \r\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\r\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\r\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\r\n",
        "        q, k, v, mask)\r\n",
        "    \r\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\r\n",
        "\r\n",
        "    concat_attention = tf.reshape(scaled_attention, \r\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\r\n",
        "\r\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\r\n",
        "        \r\n",
        "    return output, attention_weights"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxNIjeRcA3dj",
        "outputId": "32ae9ac0-1595-4ec5-f6ec-4ffc352be420"
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\r\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\r\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\r\n",
        "out.shape, attn.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q8GV_MHA7oj"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\r\n",
        "  return tf.keras.Sequential([\r\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\r\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\r\n",
        "  ])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB9hKX_qBBEN",
        "outputId": "5e0f5a06-12ca-426e-d494-decffac7568d"
      },
      "source": [
        "sample_ffn = point_wise_feed_forward_network(512, 2048)\r\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN8qMmIHBHLF"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\r\n",
        "    super(EncoderLayer, self).__init__()\r\n",
        "\r\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\r\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\r\n",
        "\r\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    \r\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\r\n",
        "    \r\n",
        "  def call(self, x, training, mask):\r\n",
        "\r\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\r\n",
        "    attn_output = self.dropout1(attn_output, training=training)\r\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\r\n",
        "    \r\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\r\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\r\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\r\n",
        "    \r\n",
        "    return out2"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOJaqVLSBMyd",
        "outputId": "f0239c76-94aa-42f9-f0d8-db4d4f1728d5"
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\r\n",
        "\r\n",
        "sample_encoder_layer_output = sample_encoder_layer(\r\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\r\n",
        "\r\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-O9itNxBR1D"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\r\n",
        "    super(DecoderLayer, self).__init__()\r\n",
        "\r\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\r\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\r\n",
        "\r\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\r\n",
        " \r\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n",
        "    \r\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\r\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\r\n",
        "    \r\n",
        "    \r\n",
        "  def call(self, x, enc_output, training, \r\n",
        "           look_ahead_mask, padding_mask):\r\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\r\n",
        "\r\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\r\n",
        "    attn1 = self.dropout1(attn1, training=training)\r\n",
        "    out1 = self.layernorm1(attn1 + x)\r\n",
        "    \r\n",
        "    attn2, attn_weights_block2 = self.mha2(\r\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\r\n",
        "    attn2 = self.dropout2(attn2, training=training)\r\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\r\n",
        "    \r\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\r\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\r\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\r\n",
        "    \r\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfUrLaGvBW0E",
        "outputId": "855e65b3-cdc1-493c-d34d-113555775633"
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\r\n",
        "\r\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\r\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \r\n",
        "    False, None, None)\r\n",
        "\r\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqFZx4ZIBg07"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\r\n",
        "               maximum_position_encoding, rate=0.1):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = d_model\r\n",
        "    self.num_layers = num_layers\r\n",
        "    \r\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\r\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \r\n",
        "                                            self.d_model)\r\n",
        "    \r\n",
        "    \r\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \r\n",
        "                       for _ in range(num_layers)]\r\n",
        "  \r\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\r\n",
        "        \r\n",
        "  def call(self, x, training, mask):\r\n",
        "\r\n",
        "    seq_len = tf.shape(x)[1]\r\n",
        "    \r\n",
        "    # adding embedding and position encoding.\r\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\r\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "    x += self.pos_encoding[:, :seq_len, :]\r\n",
        "\r\n",
        "    x = self.dropout(x, training=training)\r\n",
        "    \r\n",
        "    for i in range(self.num_layers):\r\n",
        "      x = self.enc_layers[i](x, training, mask)\r\n",
        "    \r\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Mno0w7Bqmc",
        "outputId": "87e33efc-f550-49e7-999b-513ec13d6767"
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \r\n",
        "                         dff=2048, input_vocab_size=8500,\r\n",
        "                         maximum_position_encoding=10000)\r\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\r\n",
        "\r\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\r\n",
        "\r\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 62, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QaVyQjIBvns"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\r\n",
        "               maximum_position_encoding, rate=0.1):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "\r\n",
        "    self.d_model = d_model\r\n",
        "    self.num_layers = num_layers\r\n",
        "    \r\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\r\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\r\n",
        "    \r\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \r\n",
        "                       for _ in range(num_layers)]\r\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\r\n",
        "    \r\n",
        "  def call(self, x, enc_output, training, \r\n",
        "           look_ahead_mask, padding_mask):\r\n",
        "\r\n",
        "    seq_len = tf.shape(x)[1]\r\n",
        "    attention_weights = {}\r\n",
        "    \r\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\r\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "    x += self.pos_encoding[:, :seq_len, :]\r\n",
        "    \r\n",
        "    x = self.dropout(x, training=training)\r\n",
        "\r\n",
        "    for i in range(self.num_layers):\r\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\r\n",
        "                                             look_ahead_mask, padding_mask)\r\n",
        "      \r\n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\r\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\r\n",
        "    \r\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\r\n",
        "    return x, attention_weights"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omP38OH6BzUl",
        "outputId": "030954ae-fe5c-46b2-fcf4-4611fd6ad8cd"
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \r\n",
        "                         dff=2048, target_vocab_size=8000,\r\n",
        "                         maximum_position_encoding=5000)\r\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\r\n",
        "\r\n",
        "output, attn = sample_decoder(temp_input, \r\n",
        "                              enc_output=sample_encoder_output, \r\n",
        "                              training=False,\r\n",
        "                              look_ahead_mask=None, \r\n",
        "                              padding_mask=None)\r\n",
        "\r\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoiSj9s-B4rs"
      },
      "source": [
        "class Transformer(tf.keras.Model):\r\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \r\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\r\n",
        "    super(Transformer, self).__init__()\r\n",
        "\r\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \r\n",
        "                           input_vocab_size, pe_input, rate)\r\n",
        "\r\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \r\n",
        "                           target_vocab_size, pe_target, rate)\r\n",
        "\r\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\r\n",
        "    \r\n",
        "  def call(self, inp, tar, training, enc_padding_mask, \r\n",
        "           look_ahead_mask, dec_padding_mask):\r\n",
        "\r\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\r\n",
        "    \r\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\r\n",
        "    dec_output, attention_weights = self.decoder(\r\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\r\n",
        "    \r\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\r\n",
        "    \r\n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF1MvhkeB9HE",
        "outputId": "20f8b597-f7a3-49d8-c7e6-333e69e58a3e"
      },
      "source": [
        "sample_transformer = Transformer(\r\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \r\n",
        "    input_vocab_size=8500, target_vocab_size=8000, \r\n",
        "    pe_input=10000, pe_target=6000)\r\n",
        "\r\n",
        "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\r\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\r\n",
        "\r\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \r\n",
        "                               enc_padding_mask=None, \r\n",
        "                               look_ahead_mask=None,\r\n",
        "                               dec_padding_mask=None)\r\n",
        "\r\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 36, 8000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v66ZqNOCCJc"
      },
      "source": [
        "num_layers = 4\r\n",
        "d_model = 128\r\n",
        "dff = 512\r\n",
        "num_heads = 8\r\n",
        "\r\n",
        "input_vocab_size = tokenizer_pt.vocab_size + 2\r\n",
        "target_vocab_size = tokenizer_en.vocab_size + 2\r\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-1EIubcCLc8"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n",
        "  def __init__(self, d_model, warmup_steps=4000):\r\n",
        "    super(CustomSchedule, self).__init__()\r\n",
        "    \r\n",
        "    self.d_model = d_model\r\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\r\n",
        "\r\n",
        "    self.warmup_steps = warmup_steps\r\n",
        "    \r\n",
        "  def __call__(self, step):\r\n",
        "    arg1 = tf.math.rsqrt(step)\r\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\r\n",
        "    \r\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZBGH0mPCP1d"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \r\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7i2m5PQCUcE"
      },
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\r\n",
        "\r\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\r\n",
        "plt.ylabel(\"Learning Rate\")\r\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvvOVetJCbWt"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5qPuUv6CeG1"
      },
      "source": [
        "def loss_function(real, pred):\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  loss_ = loss_object(real, pred)\r\n",
        "\r\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\r\n",
        "  loss_ *= mask\r\n",
        "  \r\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\r\n",
        "\r\n",
        "\r\n",
        "def accuracy_function(real, pred):\r\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\r\n",
        "  \r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\r\n",
        "\r\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\r\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\r\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU6t0_wtCiS7"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-idj8U9CpLd"
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\r\n",
        "                          input_vocab_size, target_vocab_size, \r\n",
        "                          pe_input=input_vocab_size, \r\n",
        "                          pe_target=target_vocab_size,\r\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFJPUQbmCuvN"
      },
      "source": [
        "def create_masks(inp, tar):\r\n",
        "  # Encoder padding mask\r\n",
        "  enc_padding_mask = create_padding_mask(inp)\r\n",
        "  \r\n",
        "  # Used in the 2nd attention block in the decoder.\r\n",
        "  # This padding mask is used to mask the encoder outputs.\r\n",
        "  dec_padding_mask = create_padding_mask(inp)\r\n",
        "  \r\n",
        "  # Used in the 1st attention block in the decoder.\r\n",
        "  # It is used to pad and mask future tokens in the input received by \r\n",
        "  # the decoder.\r\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\r\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\r\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\r\n",
        "  \r\n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOS5GOw1C0LX"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\r\n",
        "\r\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\r\n",
        "                           optimizer=optimizer)\r\n",
        "\r\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\r\n",
        "\r\n",
        "# if a checkpoint exists, restore the latest checkpoint.\r\n",
        "if ckpt_manager.latest_checkpoint:\r\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35wbRDciC9g9"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXgyZlESDA29"
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\r\n",
        "# execution. The function specializes to the precise shape of the argument\r\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\r\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\r\n",
        "# more generic shapes.\r\n",
        "\r\n",
        "train_step_signature = [\r\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n",
        "]\r\n",
        "\r\n",
        "@tf.function(input_signature=train_step_signature)\r\n",
        "def train_step(inp, tar):\r\n",
        "  tar_inp = tar[:, :-1]\r\n",
        "  tar_real = tar[:, 1:]\r\n",
        "  \r\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\r\n",
        "  \r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    predictions, _ = transformer(inp, tar_inp, \r\n",
        "                                 True, \r\n",
        "                                 enc_padding_mask, \r\n",
        "                                 combined_mask, \r\n",
        "                                 dec_padding_mask)\r\n",
        "    loss = loss_function(tar_real, predictions)\r\n",
        "\r\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \r\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n",
        "  \r\n",
        "  train_loss(loss)\r\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdxJaXoHDHUW"
      },
      "source": [
        "for epoch in range(EPOCHS):\r\n",
        "  start = time.time()\r\n",
        "  \r\n",
        "  train_loss.reset_states()\r\n",
        "  train_accuracy.reset_states()\r\n",
        "  \r\n",
        "  # inp -> portuguese, tar -> english\r\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\r\n",
        "    train_step(inp, tar)\r\n",
        "    \r\n",
        "    if batch % 50 == 0:\r\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\r\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\r\n",
        "      \r\n",
        "  if (epoch + 1) % 5 == 0:\r\n",
        "    ckpt_save_path = ckpt_manager.save()\r\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\r\n",
        "                                                         ckpt_save_path))\r\n",
        "    \r\n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \r\n",
        "                                                train_loss.result(), \r\n",
        "                                                train_accuracy.result()))\r\n",
        "\r\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt2DtKYEJrMH"
      },
      "source": [
        "def evaluate(inp_sentence):\r\n",
        "  start_token = [tokenizer_pt.vocab_size]\r\n",
        "  end_token = [tokenizer_pt.vocab_size + 1]\r\n",
        "  \r\n",
        "  # inp sentence is portuguese, hence adding the start and end token\r\n",
        "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\r\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\r\n",
        "  \r\n",
        "  # as the target is english, the first word to the transformer should be the\r\n",
        "  # english start token.\r\n",
        "  decoder_input = [tokenizer_en.vocab_size]\r\n",
        "  output = tf.expand_dims(decoder_input, 0)\r\n",
        "    \r\n",
        "  for i in range(MAX_LENGTH):\r\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\r\n",
        "        encoder_input, output)\r\n",
        "  \r\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\r\n",
        "    predictions, attention_weights = transformer(encoder_input, \r\n",
        "                                                 output,\r\n",
        "                                                 False,\r\n",
        "                                                 enc_padding_mask,\r\n",
        "                                                 combined_mask,\r\n",
        "                                                 dec_padding_mask)\r\n",
        "    \r\n",
        "    # select the last word from the seq_len dimension\r\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\r\n",
        "\r\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\r\n",
        "    \r\n",
        "    # return the result if the predicted_id is equal to the end token\r\n",
        "    if predicted_id == tokenizer_en.vocab_size+1:\r\n",
        "      return tf.squeeze(output, axis=0), attention_weights\r\n",
        "    \r\n",
        "    # concatentate the predicted_id to the output which is given to the decoder\r\n",
        "    # as its input.\r\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\r\n",
        "\r\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rQoqcE-JxI9"
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\r\n",
        "  fig = plt.figure(figsize=(16, 8))\r\n",
        "  \r\n",
        "  sentence = tokenizer_pt.encode(sentence)\r\n",
        "  \r\n",
        "  attention = tf.squeeze(attention[layer], axis=0)\r\n",
        "  \r\n",
        "  for head in range(attention.shape[0]):\r\n",
        "    ax = fig.add_subplot(2, 4, head+1)\r\n",
        "    \r\n",
        "    # plot the attention weights\r\n",
        "    ax.matshow(attention[head][:-1, :], cmap='viridis')\r\n",
        "\r\n",
        "    fontdict = {'fontsize': 10}\r\n",
        "    \r\n",
        "    ax.set_xticks(range(len(sentence)+2))\r\n",
        "    ax.set_yticks(range(len(result)))\r\n",
        "    \r\n",
        "    ax.set_ylim(len(result)-1.5, -0.5)\r\n",
        "        \r\n",
        "    ax.set_xticklabels(\r\n",
        "        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \r\n",
        "        fontdict=fontdict, rotation=90)\r\n",
        "    \r\n",
        "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \r\n",
        "                        if i < tokenizer_en.vocab_size], \r\n",
        "                       fontdict=fontdict)\r\n",
        "    \r\n",
        "    ax.set_xlabel('Head {}'.format(head+1))\r\n",
        "  \r\n",
        "  plt.tight_layout()\r\n",
        "  plt.show()"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CUQkiOhJ2O9"
      },
      "source": [
        "def translate(sentence, plot=''):\r\n",
        "  result, attention_weights = evaluate(sentence)\r\n",
        "  \r\n",
        "  predicted_sentence = tokenizer_en.decode([i for i in result \r\n",
        "                                            if i < tokenizer_en.vocab_size])  \r\n",
        "\r\n",
        "  print('Input: {}'.format(sentence))\r\n",
        "  print('Predicted translation: {}'.format(predicted_sentence))\r\n",
        "  \r\n",
        "  if plot:\r\n",
        "    plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcGV0M9EJ9cI",
        "outputId": "9f43cc89-6b7c-44b3-ae84-eb68f3977e6b"
      },
      "source": [
        "translate(\"este é um problema que temos que resolver.\")\r\n",
        "print (\"Real translation: this is a problem we have to solve .\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: este é um problema que temos que resolver.\n",
            "Predicted translation: this is a problem that we have to solve the united states .\n",
            "Real translation: this is a problem we have to solve .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "TZ3KpcDbKQOx",
        "outputId": "96ee9078-c66f-44f9-8333-2dd956a665c5"
      },
      "source": [
        "translate(\"este é o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\r\n",
        "print (\"Real translation: this is the first book i've ever done.\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: este é o primeiro livro que eu fiz.\n",
            "Predicted translation: this is the first book i did .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Real translation: this is the first book i've ever done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}